{"version":"1","records":[{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook"},"content":"{width=“300”}\n\n\n\n\n\n\n\n\n\n📚 See the \n\nCookbook Contributor’s Guide for step-by-step instructions on how to create your new Cookbook and get it hosted on the \n\nPythia Cookbook Gallery!\n\nThis Project Pythia Cookbook covers how geoscience enables algorithmic literacy through AI-guided microburst learning. In this application, learners gain core skills and confidence, earning badges toward data credentials. Most importantly, they reinvest by mentoring peers, co-creating local data solutions, and advocating for equitable systems—amplifying community voice and resilience through algorithmic fluency.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"🌍 Enhancing Algorithmic Literacy Among Vulnerable Populations through Geoscience Applications"},"type":"lvl2","url":"/#id-enhancing-algorithmic-literacy-among-vulnerable-populations-through-geoscience-applications","position":2},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"🌍 Enhancing Algorithmic Literacy Among Vulnerable Populations through Geoscience Applications"},"content":"Imagine a community-based geoscience project where local learners explore biodiversity through a dataset of penguin species.\n\nIn the geoscience community, algorithm literacy is more than technical fluency—it’s a pathway to justice. Vulnerable populations often bear the brunt of environmental decisions shaped by opaque data systems. To shift this paradigm, we must equip these communities not only with knowledge, but with agency.\n\n🤖 Enter the AI teammate: not as a distant tool, but as a responsive partner! Through explicit AI prompts, learners can engage in targeted, conversational learning that breaks down complex algorithmic concepts into accessible, actionable insights. These prompts serve as microburst upskilling lessons—short, focused interactions where AI explains concepts like correlations, classification, and feature importance in plain language.\n\nVulnerable populations, often excluded from technical discourse, now gain the tools to explore, question, and interpret data that mirrors real-world ecological systems—tailored to real-world geoscience challenges.\n\nBy embedding microburst upskilling into daily workflows and community initiatives, we create rapid, scalable opportunities for learning. These short, focused interactions with AI empower users to ask better questions, challenge assumptions, and co-create solutions. The result? A growing network of algorithm-literate individuals who can actively participate in—and shape—their own liberation.\n\nLet us reimagine algorithm literacy not as a distant goal, but as a daily practice. With AI as a teammate and microburst learning as our method, we can build a geoscience future that is inclusive, transparent, and driven by the voices that matter most.","type":"content","url":"/#id-enhancing-algorithmic-literacy-among-vulnerable-populations-through-geoscience-applications","position":3},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"✍️ Authors"},"type":"lvl2","url":"/#id-authors","position":4},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"✍️ Authors"},"content":"Connor Quiroz, \n\nLead Author; Temmy Adesole, \n\nFirst Author; Tiffany Boyer, CMfgE - \n\nSecond Author; Dr. Francis Tuluiri, PhD - \n\nThird Author","type":"content","url":"/#id-authors","position":5},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"🙌 Contributors","lvl2":"✍️ Authors"},"type":"lvl3","url":"/#id-contributors","position":6},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"🙌 Contributors","lvl2":"✍️ Authors"},"content":"","type":"content","url":"/#id-contributors","position":7},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"🧭 Structure"},"type":"lvl2","url":"/#id-structure","position":8},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"🧭 Structure"},"content":"(State one or more sections that will comprise the notebook. E.g., This cookbook is broken up into two main sections - “Foundations” and “Example Workflows.” Then, describe each section below.)","type":"content","url":"/#id-structure","position":9},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"📚 Section 1 (Replace with the title of this section, e.g. “Foundations”)","lvl2":"🧭 Structure"},"type":"lvl3","url":"/#id-section-1-replace-with-the-title-of-this-section-e-g-foundations","position":10},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"📚 Section 1 (Replace with the title of this section, e.g. “Foundations”)","lvl2":"🧭 Structure"},"content":"(Add content for this section, e.g., “The foundational content includes ...”)","type":"content","url":"/#id-section-1-replace-with-the-title-of-this-section-e-g-foundations","position":11},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"🛠️ Section 2 (Replace with the title of this section, e.g. “Example workflows”)","lvl2":"🧭 Structure"},"type":"lvl3","url":"/#id-section-2-replace-with-the-title-of-this-section-e-g-example-workflows","position":12},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"🛠️ Section 2 (Replace with the title of this section, e.g. “Example workflows”)","lvl2":"🧭 Structure"},"content":"(Add content for this section, e.g., “Example workflows include ...”)","type":"content","url":"/#id-section-2-replace-with-the-title-of-this-section-e-g-example-workflows","position":13},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"🧪 Running the Notebooks"},"type":"lvl2","url":"/#id-running-the-notebooks","position":14},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"🧪 Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#id-running-the-notebooks","position":15},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"🚀 Running on Binder","lvl2":"🧪 Running the Notebooks"},"type":"lvl3","url":"/#id-running-on-binder","position":16},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"🚀 Running on Binder","lvl2":"🧪 Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through \n\nBinder, which enables the execution of a \n\nJupyter Book in the cloud. The details of how this works are not important for now. All you need to know is how to launch a Pythia Cookbooks chapter via Binder.\n\nSimply navigate your mouse to the top right corner of the book chapter you are viewing and click on the 🚀 rocket ship icon, and be sure to select “launch Binder”. After a moment you should be presented with a notebook that you can interact with. You’ll be able to execute and even change the example programs. You’ll see that the code cells have no output at first, until you execute them by pressing {kbd}Shift+Enter.\n\n📖 Complete details on how to interact with a live Jupyter notebook are described in \n\nGetting Started with Jupyter.\n\n⚠️ Note: Not all Cookbook chapters are executable. If you do not see the rocket ship icon, such as on this page, you are not viewing an executable book chapter.","type":"content","url":"/#id-running-on-binder","position":17},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"🖥️ Running on Your Own Machine","lvl2":"🧪 Running the Notebooks"},"type":"lvl3","url":"/#id-running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"📘 A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"🖥️ Running on Your Own Machine","lvl2":"🧪 Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)# 1. Clone the repository\ngit clone https://github.com/ProjectPythia/cookbook-example.git\n\n# 2. Move into the directory\ncd cookbook-example\n\n# 3. Create and activate your conda environment\nconda env create -f environment.yml\nconda activate cookbook-example\n\n# 4. Start JupyterLab\ncd notebooks/\njupyter lab\n\ndigital literacy and data fluency empowering broader partication in the data-driven ecosystem.\n\n### Conclusion\n\nThis project demonstrates that data science is accessible and impactful across disciplines--from research and industry to government and community initiatives. By emphasizing hands-on experience, targeted prompts, and reproducible workflows, learners navigate their learning pathways through the attainment of task-specific badges that build toward meaningful data interpetation, problem solving, and critial thinking credentials. \n\nNo prior coding is required to engage meaningfully with data science; and at its highest application learners gain and subsequently share core skills through open-sourced upskilling experiences that bridge the gap between technical barriers and real-world applications, empowering broader participation in the data-driven economy.  Crucially, learners re-invest by sharing the results of their skills development, co-creating local open-science solutions, and advocating for equity.  amplifying community voice and resilience through algorithmic fluency.\n\nA key takeaway from our approach is that through AI-supported upskilling, learners can bridge the gap between technical barriers and real-world applications, empowering broader participation in the data-driven economy.  Most importantly,collective,open sourced content sharing can amplify community voice and resilience through algorithmic fluency.\n\nUltimately, this approach advocates for a democratized vision of data science—where anyone can learn, practice, and contribute—by fostering inclusive learning environments, sharing reproducible tools, and promoting AI-integrated pathways e.g. the AI Integration Specialist badge. \n","type":"content","url":"/#id-running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization"},"type":"lvl1","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization","position":0},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization","position":1},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"AI-Powered Data Cleaning & Visualization with Palmer Penguins"},"type":"lvl3","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#ai-powered-data-cleaning-visualization-with-palmer-penguins","position":2},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"AI-Powered Data Cleaning & Visualization with Palmer Penguins"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#ai-powered-data-cleaning-visualization-with-palmer-penguins","position":3},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🐧 Welcome to your AI Integration Journey!","lvl3":"AI-Powered Data Cleaning & Visualization with Palmer Penguins"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-welcome-to-your-ai-integration-journey","position":4},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🐧 Welcome to your AI Integration Journey!","lvl3":"AI-Powered Data Cleaning & Visualization with Palmer Penguins"},"content":"This section demonstrates how AI can enhance every step of your data science workflow, from initial data cleaning to creating compelling visualizations. You’ll learn the WHAT, HOW, and WHY of integrating AI into your data science process.\n\nBy the end of this section, you’ll understand:\n\nHow to collaborate with AI for data exploration\n\nAI-assisted data cleaning techniques\n\nCreating publication-ready visualizations with AI guidance\n\nBest practices for human-AI collaboration in data science","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-welcome-to-your-ai-integration-journey","position":5},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🎯 The WHAT, HOW, and WHY of AI Integration"},"type":"lvl3","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-the-what-how-and-why-of-ai-integration","position":6},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🎯 The WHAT, HOW, and WHY of AI Integration"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-the-what-how-and-why-of-ai-integration","position":7},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"📖 WHAT: Understanding AI in Data Science","lvl3":"🎯 The WHAT, HOW, and WHY of AI Integration"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-what-understanding-ai-in-data-science","position":8},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"📖 WHAT: Understanding AI in Data Science","lvl3":"🎯 The WHAT, HOW, and WHY of AI Integration"},"content":"Important: AI integration in data science isn’t about replacing human expertise—it’s about amplifying your capabilities. Think of AI as your intelligent collaborator that brings:\n\n🔍 Smart Research Assistant: Helps explore data patterns\n\n🧹 Intelligent Cleaner: Identifies and handles data quality issues\n\n🎨 Visualization Designer: Suggests optimal chart types and styling\n\n🤖 Pattern Detective: Reveals hidden insights in your data","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-what-understanding-ai-in-data-science","position":9},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"Key Integration Points:","lvl3":"🎯 The WHAT, HOW, and WHY of AI Integration"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#key-integration-points","position":10},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"Key Integration Points:","lvl3":"🎯 The WHAT, HOW, and WHY of AI Integration"},"content":"Data Loading & Initial Exploration\n\nMissing Value Detection & Treatment\n\nOutlier Analysis & Handling\n\nVisualization Strategy & Implementation\n\nInsight Generation & Interpretation\n\nPrompts we gave to the AI chat\n\nChoose a chatbot of your choice (e.g., ChatGPT, Claude, Copilot, Gemini) to begin typing prompts for your data science project. Here are examples we used to clean and visualize our data:\n\nUsing GitHub, load the Palmer Penguins dataset and display the variable names. Let me create a Python script that does this.\n\nGIVEN these variables in the dataset, can you create 3 plots useful for differentiating these species that use only five of the variables? Use only pandas, matplotlib, and plotly.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\nimport seaborn as sns\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up plotting preferences\nplt.style.use('seaborn-v0_8-whitegrid')\npio.templates.default = \"plotly_white\"\n\nprint(\"📚 Libraries loaded successfully!\")\n\ndef load_penguin_data_with_ai_insights():\n    \"\"\"\n    AI-Enhanced data loading function that provides immediate insights\n    \"\"\"\n    try:\n        # GitHub URL for the CSV file (raw format)\n        url = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n        \n        print(\"📥 Loading Palmer Penguins dataset...\")\n        penguins = pd.read_csv(url)\n        \n        # AI-generated immediate insights\n        print(f\"✅ Successfully loaded {len(penguins)} penguin observations\")\n        print(f\"📊 Dataset contains {penguins.shape[1]} variables\")\n        print(f\"🏝️  Data collected from {penguins['island'].nunique()} islands\")\n        print(f\"🐧 Includes {penguins['species'].nunique()} penguin species\")\n        print(f\"📅 Covers years {penguins['year'].min()}-{penguins['year'].max()}\")\n        \n        return penguins\n        \n    except Exception as e:\n        print(f\"❌ Error loading data: {e}\")\n        print(\"💡 AI Suggestion: Check internet connection and URL accessibility\")\n        return None\n\n# Execute AI-enhanced data loading\npenguins = load_penguin_data_with_ai_insights()\n\n","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#key-integration-points","position":11},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"💡 Why This Approach Works:","lvl3":"🎯 The WHAT, HOW, and WHY of AI Integration"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-why-this-approach-works","position":12},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"💡 Why This Approach Works:","lvl3":"🎯 The WHAT, HOW, and WHY of AI Integration"},"content":"Notice how we’re not just loading data—we’re immediately getting contextual insights that inform our next steps. The AI helps us understand the scope and structure before we dive deeper.\n\ndef generate_ai_data_profile(df):\n“”\"\nAI-enhanced data profiling that goes beyond basic statistics\n“”\"\nprint(“🔍 AI-Generated Data Profile Report”)\nprint(“=” * 40)# Basic structure\nprint(f\"📋 Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\nprint(f\"💾 Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\nprint()\n\n# Variable types analysis\nprint(\"📊 Variable Type Analysis:\")\ndtype_counts = df.dtypes.value_counts()\nfor dtype, count in dtype_counts.items():\n    print(f\"   • {dtype}: {count} variables\")\nprint()\n\n# Missing data analysis\nmissing_data = df.isnull().sum()\nmissing_pct = (missing_data / len(df)) * 100\n\nprint(\"🔍 Missing Data Analysis:\")\nif missing_data.sum() == 0:\n    print(\"   ✅ No missing values detected!\")\nelse:\n    print(\"   ⚠️  Missing values found:\")\n    for col in missing_data[missing_data > 0].index:\n        print(f\"      • {col}: {missing_data[col]} ({missing_pct[col]:.1f}%)\")\nprint()\n\n# Data quality flags - AI-powered quality assessment\nprint(\"🚩 AI Data Quality Flags:\")\nquality_flags = []\n\n# Check for potential duplicates\nduplicate_count = df.duplicated().sum()\nif duplicate_count > 0:\n    quality_flags.append(f\"Potential duplicates: {duplicate_count}\")\n\n# Check for outliers in numeric columns\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\nfor col in numeric_cols:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = len(df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)])\n    if outliers > 0:\n        quality_flags.append(f\"{col}: {outliers} potential outliers\")\n\nif quality_flags:\n    for flag in quality_flags:\n        print(f\"   ⚠️  {flag}\")\nelse:\n    print(\"   ✅ No major quality issues detected\")\n\nreturn missing_data, missing_pct","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-why-this-approach-works","position":13},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl2":"Generate AI data profile"},"type":"lvl2","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#generate-ai-data-profile","position":14},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl2":"Generate AI data profile"},"content":"missing_summary, missing_percentages = generate_ai_data_profile(penguins)\n\n# Display the data structure\nprint(\"📋 Dataset Structure:\")\nprint(\"-\" * 20)\ndisplay(penguins.head(10))\nprint()\n\nprint(\"🏷️ Variable Definitions:\")\nprint(\"-\" * 25)\nvariable_definitions = {\n    'species': 'Penguin species (Adelie, Chinstrap, Gentoo)',\n    'island': 'Island where observed (Biscoe, Dream, Torgersen)', \n    'bill_length_mm': 'Bill length measurement in millimeters',\n    'bill_depth_mm': 'Bill depth measurement in millimeters',\n    'flipper_length_mm': 'Flipper length measurement in millimeters',\n    'body_mass_g': 'Body mass measurement in grams',\n    'sex': 'Penguin sex (male, female)',\n    'year': 'Year of observation (2007-2009)'\n}\n\nfor var, definition in variable_definitions.items():\n    print(f\"• {var}: {definition}\")\n\n","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#generate-ai-data-profile","position":15},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🧹 AI-Assisted Data Cleaning","lvl2":"Generate AI data profile"},"type":"lvl3","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-ai-assisted-data-cleaning","position":16},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🧹 AI-Assisted Data Cleaning","lvl2":"Generate AI data profile"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-ai-assisted-data-cleaning","position":17},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🤖 How We Engage AI for Intelligent Cleaning","lvl3":"🧹 AI-Assisted Data Cleaning","lvl2":"Generate AI data profile"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-how-we-engage-ai-for-intelligent-cleaning","position":18},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🤖 How We Engage AI for Intelligent Cleaning","lvl3":"🧹 AI-Assisted Data Cleaning","lvl2":"Generate AI data profile"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-how-we-engage-ai-for-intelligent-cleaning","position":19},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl5":"💬 Effective AI Prompting for Data Cleaning","lvl4":"🤖 How We Engage AI for Intelligent Cleaning","lvl3":"🧹 AI-Assisted Data Cleaning","lvl2":"Generate AI data profile"},"type":"lvl5","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-effective-ai-prompting-for-data-cleaning","position":20},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl5":"💬 Effective AI Prompting for Data Cleaning","lvl4":"🤖 How We Engage AI for Intelligent Cleaning","lvl3":"🧹 AI-Assisted Data Cleaning","lvl2":"Generate AI data profile"},"content":"Instead of: “Clean my data”Try: “Analyze the missing data patterns and recommend a cleaning strategy that balances data retention with model performance for a penguin species classification task”\n\nWhy it works: We provide context (classification task), constraints (data retention vs performance), and specific guidance (missing data patterns).\n\nKey Insight: The key to AI-assisted cleaning is providing domain context and analytical objectives\n\ndef ai_assisted_data_cleaning(df):\n    \"\"\"\n    AI-guided data cleaning with explanations for each step\n    \"\"\"\n    print(\"🧹 AI Data Cleaning Assistant Activated\")\n    print(\"=\" * 42)\n    \n    # Create a copy for cleaning\n    clean_df = df.copy()\n    original_rows = len(clean_df)\n    \n    print(f\"📊 Starting with {original_rows:,} observations\")\n    print()\n    \n    # Step 1: Missing value analysis and treatment\n    print(\"Step 1: 🔍 Missing Value Analysis\")\n    print(\"-\" * 35)\n    \n    missing_counts = clean_df.isnull().sum()\n    if missing_counts.sum() > 0:\n        print(\"Missing values detected:\")\n        for col in missing_counts[missing_counts > 0].index:\n            count = missing_counts[col]\n            pct = (count / len(clean_df)) * 100\n            print(f\"  • {col}: {count} values ({pct:.1f}%)\")\n        \n        print(\"\\n🤖 AI Recommendation: Remove rows with ANY missing values\")\n        print(\"   Rationale: For classification tasks, complete cases provide\")\n        print(\"   the most reliable training data. Small dataset allows this approach.\")\n        \n        # Apply cleaning\n        clean_df = clean_df.dropna()\n        removed_rows = original_rows - len(clean_df)\n        print(f\"\\n✅ Removed {removed_rows} rows with missing values\")\n        print(f\"📊 Clean dataset: {len(clean_df):,} observations\")\n    else:\n        print(\"✅ No missing values found!\")\n    \n    print()\n    \n    # Step 2: Data type optimization\n    print(\"Step 2: 🔧 Data Type Optimization\") \n    print(\"-\" * 35)\n    \n    # Convert categorical variables\n    categorical_vars = ['species', 'island', 'sex']\n    for var in categorical_vars:\n        if var in clean_df.columns:\n            clean_df[var] = clean_df[var].astype('category')\n            print(f\"  • {var}: converted to categorical\")\n    \n    print(\"✅ Data types optimized for memory efficiency\")\n    print()\n    \n    # Step 3: Outlier detection\n    print(\"Step 3: 🎯 Outlier Analysis\")\n    print(\"-\" * 35)\n    \n    numeric_cols = clean_df.select_dtypes(include=[np.number]).columns\n    outlier_summary = {}\n    \n    for col in numeric_cols:\n        if col != 'year':  # Skip year column\n            Q1 = clean_df[col].quantile(0.25)\n            Q3 = clean_df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            \n            outliers = clean_df[(clean_df[col] < lower_bound) | \n                              (clean_df[col] > upper_bound)]\n            \n            outlier_summary[col] = len(outliers)\n            \n            if len(outliers) > 0:\n                print(f\"  • {col}: {len(outliers)} potential outliers\")\n            else:\n                print(f\"  ✅ {col}: No outliers detected\")\n    \n    print(\"\\n🤖 AI Recommendation: Keep outliers for biological diversity\")\n    print(\"   Rationale: In biological data, extreme values often represent\")\n    print(\"   natural variation rather than measurement errors.\")\n    print()\n    \n    # Final summary\n    print(\"📋 Cleaning Summary:\")\n    print(\"-\" * 20)\n    print(f\"Original rows: {original_rows:,}\")\n    print(f\"Final rows: {len(clean_df):,}\")\n    print(f\"Data retention: {(len(clean_df)/original_rows)*100:.1f}%\")\n    \n    return clean_df, outlier_summary\n\n# Execute AI-assisted cleaning\nclean_penguins, outlier_info = ai_assisted_data_cleaning(penguins)\n\n","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-effective-ai-prompting-for-data-cleaning","position":21},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🧠 AI Cleaning Philosophy","lvl3":"🧹 AI-Assisted Data Cleaning","lvl2":"Generate AI data profile"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-ai-cleaning-philosophy","position":22},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🧠 AI Cleaning Philosophy","lvl3":"🧹 AI-Assisted Data Cleaning","lvl2":"Generate AI data profile"},"content":"Notice how the AI provides not just actions, but rationale for each decision. This builds trust and understanding, making the process educational rather than just automated.","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-ai-cleaning-philosophy","position":23},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🎨 AI-Enhanced Data Visualization","lvl2":"Generate AI data profile"},"type":"lvl3","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-ai-enhanced-data-visualization","position":24},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🎨 AI-Enhanced Data Visualization","lvl2":"Generate AI data profile"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-ai-enhanced-data-visualization","position":25},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🤖 Engaging AI for Strategic Visualization Planning","lvl3":"🎨 AI-Enhanced Data Visualization","lvl2":"Generate AI data profile"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-engaging-ai-for-strategic-visualization-planning","position":26},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🤖 Engaging AI for Strategic Visualization Planning","lvl3":"🎨 AI-Enhanced Data Visualization","lvl2":"Generate AI data profile"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-engaging-ai-for-strategic-visualization-planning","position":27},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl5":"💬 How to Ask AI for Visualization Strategy","lvl4":"🤖 Engaging AI for Strategic Visualization Planning","lvl3":"🎨 AI-Enhanced Data Visualization","lvl2":"Generate AI data profile"},"type":"lvl5","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-how-to-ask-ai-for-visualization-strategy","position":28},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl5":"💬 How to Ask AI for Visualization Strategy","lvl4":"🤖 Engaging AI for Strategic Visualization Planning","lvl3":"🎨 AI-Enhanced Data Visualization","lvl2":"Generate AI data profile"},"content":"Instead of: “Make some charts”Try: “Given that I have a classification problem with 3 species and 4 numerical features, what visualization strategy would best reveal class separability and feature relationships?”\n\nResult: AI provides a strategic framework rather than random charts.\n\ndef ai_visualization_strategy(df):\n    \"\"\"\n    AI-powered visualization recommendations based on data characteristics\n    \"\"\"\n    print(\"🎨 AI Visualization Strategist\")\n    print(\"=\" * 32)\n    \n    # Analyze data characteristics\n    n_categorical = len(df.select_dtypes(include=['category', 'object']).columns)\n    n_numerical = len(df.select_dtypes(include=[np.number]).columns) - 1  # Exclude year\n    n_observations = len(df)\n    \n    print(f\"📊 Data Profile: {n_observations} obs, {n_categorical} categorical, {n_numerical} numerical variables\")\n    print()\n    \n    print(\"🎯 AI-Recommended Visualization Strategy:\")\n    print(\"-\" * 42)\n    \n    recommendations = [\n        {\n            'viz_type': 'Species Distribution Analysis',\n            'purpose': 'Understand class balance in our target variable',\n            'ai_rationale': 'Essential for classification - reveals potential class imbalance issues'\n        },\n        {\n            'viz_type': 'Bivariate Relationship Exploration', \n            'purpose': 'Discover feature relationships and species separation',\n            'ai_rationale': 'Scatter plots reveal natural clustering and decision boundaries'\n        },\n        {\n            'viz_type': 'Feature Distribution Comparison',\n            'purpose': 'Compare distributions across species',\n            'ai_rationale': 'Box plots show central tendency and variability for each group'\n        },\n        {\n            'viz_type': 'Correlation Heatmap',\n            'purpose': 'Identify multicollinearity and feature relationships',\n            'ai_rationale': 'Critical for feature selection and model interpretation'\n        }\n    ]\n    \n    for i, rec in enumerate(recommendations, 1):\n        print(f\"{i}. {rec['viz_type']}\")\n        print(f\"   Purpose: {rec['purpose']}\")\n        print(f\"   🤖 AI Rationale: {rec['ai_rationale']}\")\n        print()\n    \n    return recommendations\n\n# Get AI visualization strategy\nviz_strategy = ai_visualization_strategy(clean_penguins)\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Count species\nspecies_counts = clean_penguins['species'].value_counts()\nspecies = species_counts.index.tolist()\ncounts = species_counts.values\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n\n# Create subplot layout with 1 row, 2 columns\nfig = make_subplots(rows=1, cols=2, \n                    subplot_titles=(\"Penguin Species Distribution\", \"Species Proportion\"),\n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"domain\"}]])\n\n# Bar chart\nfig.add_trace(\n    go.Bar(\n        x=species,\n        y=counts,\n        text=counts,\n        textposition='outside',\n        marker_color=colors,\n        name=\"Count\"\n    ),\n    row=1, col=1\n)\n\n# Pie chart\nfig.add_trace(\n    go.Pie(\n        labels=species,\n        values=counts,\n        marker_colors=colors,\n        name=\"Proportion\",\n        hole=0,\n        textinfo='label+percent'\n    ),\n    row=1, col=2\n)\n\n# Update layout\nfig.update_layout(\n    height=500,\n    width=900,\n    title_text=\"📊 Visualization 1: Species Distribution Analysis\",\n    title_font_size=18,\n    showlegend=False,\n    margin=dict(t=80)\n)\n\nfig.show()\n\n# Print AI-Generated Insights\nprint(f\"\\n🤖 AI-Generated Insights:\")\ntotal = len(clean_penguins)\nfor sp in species:\n    count = species_counts[sp]\n    percentage = count / total * 100\n    print(f\"• {sp}: {count} penguins ({percentage:.1f}%)\")\n\n\nimport plotly.express as px\nimport plotly.subplots as sp\nimport plotly.graph_objects as go\n\n# Relationships to explore\nrelationships = [\n    ('bill_length_mm', 'bill_depth_mm'),\n    ('bill_length_mm', 'flipper_length_mm'),\n    ('flipper_length_mm', 'body_mass_g'),\n    ('bill_depth_mm', 'body_mass_g')\n]\n\n# Color mapping\ncolors = {'Adelie': '#FF6B6B', 'Chinstrap': '#4ECDC4', 'Gentoo': '#45B7D1'}\n\n# Create subplot grid: 2 rows, 2 columns\nfig = sp.make_subplots(rows=2, cols=2,\n                       subplot_titles=[f'{x.replace(\"_\", \" \").title()} vs {y.replace(\"_\", \" \").title()}' \n                                       for x, y in relationships])\n\n# Add scatter traces for each relationship and species\nfor idx, (x_var, y_var) in enumerate(relationships):\n    row = idx // 2 + 1\n    col = idx % 2 + 1\n\n    for species in clean_penguins['species'].unique():\n        species_data = clean_penguins[clean_penguins['species'] == species]\n\n        fig.add_trace(\n            go.Scatter(\n                x=species_data[x_var],\n                y=species_data[y_var],\n                mode='markers',\n                name=species if idx == 0 else None,  # Show legend only once\n                marker=dict(color=colors[species], size=8, opacity=0.7),\n                legendgroup=species,\n                showlegend=(idx == 0),\n                hovertemplate=f'<b>{species}</b><br>{x_var}: %{{x}}<br>{y_var}: %{{y}}<extra></extra>'\n            ),\n            row=row, col=col\n        )\n\n# Update layout\nfig.update_layout(\n    height=800,\n    width=1000,\n    title_text=\"📊 AI-Recommended Feature Relationships by Species\",\n    title_font=dict(size=18, family='Arial', color='black'),\n    plot_bgcolor='white',\n    legend_title_text='Species',\n    margin=dict(t=80)\n)\n\n# Update axis titles\nfor idx, (x_var, y_var) in enumerate(relationships):\n    row = idx // 2 + 1\n    col = idx % 2 + 1\n    fig.update_xaxes(title_text=x_var.replace('_', ' ').title(), row=row, col=col, showgrid=True, gridcolor='lightgray')\n    fig.update_yaxes(title_text=y_var.replace('_', ' ').title(), row=row, col=col, showgrid=True, gridcolor='lightgray')\n\n# Show figure\nfig.show()\n\n# Print pattern recognition notes\nprint(\"\\n🤖 AI Pattern Recognition:\")\nprint(\"• Clear species clustering visible in multiple feature combinations\")\nprint(\"• Excellent linear separability suggests high classification accuracy potential\")\nprint(\"• Flipper length vs body mass shows strongest species separation\")\n\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nprint(\"📊 Visualization 3: Feature Distribution Comparison\")\nprint(\"-\" * 52)\n\n# Variables and color setup\nnumerical_vars = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\nspecies_categories = clean_penguins['species'].cat.categories\ncolors = {'Adelie': '#FF6B6B', 'Chinstrap': '#4ECDC4', 'Gentoo': '#45B7D1'}\n\n# Create 2x2 subplot grid\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=[var.replace('_', ' ').title() for var in numerical_vars],\n    horizontal_spacing=0.15,\n    vertical_spacing=0.2\n)\n\n# Add box plots\nfor idx, var in enumerate(numerical_vars):\n    row = idx // 2 + 1\n    col = idx % 2 + 1\n    for species in species_categories:\n        fig.add_trace(\n            go.Box(\n                x=[species] * len(clean_penguins[clean_penguins['species'] == species]),\n                y=clean_penguins[clean_penguins['species'] == species][var],\n                name=species,\n                boxmean=True,\n                width=0.4,  # Wider box\n                marker_color=colors[species],\n                legendgroup=species,\n                showlegend=(idx == 0),  # Show legend only once\n                line=dict(width=1),\n                opacity=0.7\n            ),\n            row=row, col=col\n        )\n    # Lock x-axis category order for consistency\n    fig.update_xaxes(\n        categoryorder='array',\n        categoryarray=list(species_categories),\n        title_text=\"Species\",\n        row=row, col=col\n    )\n\n    fig.update_yaxes(title_text=\"Value\", row=row, col=col)\n\n# Layout tweaks\nfig.update_layout(\n    height=800,\n    width=1000,\n    title_text=\"📊 Feature Distributions by Species\",\n    title_font=dict(size=18, family='Arial', color='black'),\n    margin=dict(t=80),\n    boxmode='group'\n)\n\nfig.show()\n\n# AI-generated insights\nprint(\"\\n🤖 AI Species Profiling:\")\nprint(\"• Adelie: Shorter bills, deeper bills, smaller overall size\")\nprint(\"• Chinstrap: Longest bills, medium size\")\nprint(\"• Gentoo: Longest flippers, largest body mass\")\nprint(\"• Clear discriminative patterns identified for classification\")\n\nprint(\"📊 Visualization 4: Interactive Feature Explorer\")\nprint(\"-\" * 50)\n\n# Create interactive scatter plot\nfig = px.scatter(clean_penguins,\n                x='flipper_length_mm',\n                y='body_mass_g',\n                color='species',\n                size='bill_length_mm',\n                hover_data=['bill_depth_mm', 'island', 'sex'],\n                title='Interactive Penguin Feature Explorer',\n                color_discrete_map={'Adelie': '#FF6B6B', \n                                   'Chinstrap': '#4ECDC4', \n                                   'Gentoo': '#45B7D1'})\n\nfig.update_layout(\n    height=600,\n    title_font_size=16,\n    xaxis_title=\"Flipper Length (mm)\",\n    yaxis_title=\"Body Mass (g)\"\n)\n\n# Display the plot\nfig.show()\n\nprint(\"\\n🤖 Interactive Analysis Complete!\")\nprint(\"• Hover over points to explore individual penguin characteristics\")\nprint(\"• Notice the clear species clustering in flipper-mass space\")\nprint(\"• Bill length (bubble size) correlates strongly with body mass\")\n\n","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-how-to-ask-ai-for-visualization-strategy","position":29},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🏆 Key Takeaways and Next Steps","lvl2":"Generate AI data profile"},"type":"lvl3","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-key-takeaways-and-next-steps","position":30},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🏆 Key Takeaways and Next Steps","lvl2":"Generate AI data profile"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-key-takeaways-and-next-steps","position":31},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🎯 What You’ve Accomplished","lvl3":"🏆 Key Takeaways and Next Steps","lvl2":"Generate AI data profile"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-what-youve-accomplished","position":32},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🎯 What You’ve Accomplished","lvl3":"🏆 Key Takeaways and Next Steps","lvl2":"Generate AI data profile"},"content":"✅ Your AI Integration Achievements:\n\nMastered AI-enhanced data loading with immediate insights\n\nGenerated comprehensive data profiles using AI assistance\n\nImplemented intelligent data cleaning with AI recommendations\n\nCreated strategic visualizations guided by AI analysis\n\nDiscovered key patterns that will drive successful classification","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-what-youve-accomplished","position":33},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🧠 Key AI Integration Principles","lvl3":"🏆 Key Takeaways and Next Steps","lvl2":"Generate AI data profile"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-key-ai-integration-principles","position":34},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🧠 Key AI Integration Principles","lvl3":"🏆 Key Takeaways and Next Steps","lvl2":"Generate AI data profile"},"content":"🤝 Collaboration over Replacement - AI amplifies human expertise rather than replacing it\n\n📊 Data-Driven Decisions - Let AI analyze patterns, humans interpret context\n\n🔄 Iterative Improvement - Use AI feedback to refine your approach\n\n📈 Scalable Methods - Develop reusable AI-assisted workflows\n\n🎯 Goal-Oriented - Align AI assistance with business objectives","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-key-ai-integration-principles","position":35},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🚀 Ready for Section 2: Advanced AI Model Building","lvl2":"Generate AI data profile"},"type":"lvl3","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-ready-for-section-2-advanced-ai-model-building","position":36},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl3":"🚀 Ready for Section 2: Advanced AI Model Building","lvl2":"Generate AI data profile"},"content":"","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-ready-for-section-2-advanced-ai-model-building","position":37},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🎖️ Achievement Unlocked!","lvl3":"🚀 Ready for Section 2: Advanced AI Model Building","lvl2":"Generate AI data profile"},"type":"lvl4","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-achievement-unlocked","position":38},{"hierarchy":{"lvl1":"Section 1 - Data Cleaning / Data Visualization","lvl4":"🎖️ Achievement Unlocked!","lvl3":"🚀 Ready for Section 2: Advanced AI Model Building","lvl2":"Generate AI data profile"},"content":"Congratulations! You’ve earned the ‘AI Data Explorer’ badge!\n\nIn the next section, you’ll learn to:\n\nBuild and train classification models with AI guidance\n\nOptimize model performance using AI recommendations\n\nDeploy your trained models for real-world predictions\n\nEvaluate and interpret results with AI assistance\n\nNote: Continue to Section 2 to become an ‘AI Model Builder’ and complete your journey to AI Integration Expert!\n\n# Save cleaned data for next section\nclean_penguins.to_csv('cleaned_penguins_data.csv', index=False)\nprint(\"💾 Cleaned dataset saved as 'cleaned_penguins_data.csv' for Section 2\")\nprint(f\"📊 Final dataset contains {len(clean_penguins)} observations\")\nprint(\"🚀 Ready to proceed to AI Model Building!\")\n\n{\n    \"tags\": [\n        \"hide-cell\"\n    ]\n}","type":"content","url":"/notebooks/chapter-1-data-cleaning-and-data-visualization#id-achievement-unlocked","position":39},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/notebooks/ftuluri01","position":0},{"hierarchy":{"lvl1":""},"content":"import pandas as pd\n\n# GitHub URL for the CSV file (raw format)\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n\n# Load the dataset into a pandas DataFrame\npenguins = pd.read_csv(url)\n\n# Display the variable (column) names\nprint(\"Variable (column) names in the Palmer Penguins dataset:\")\nprint(penguins.columns.tolist())\n\npip install plotly\n\nimport plotly\n\nimport pandas as pd\n\n# GitHub URL for the CSV file (raw format)\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n\n# Load the dataset into a pandas DataFrame\npenguins = pd.read_csv(url)\n\n# Display the variable (column) names\nprint(\"Variable (column) names in the Palmer Penguins dataset:\")\nprint(penguins.columns.tolist())\n\n\nprint(penguins.head(10))\n\ncolumn_names_list = penguins.columns.tolist()\nprint(\"Column names (as a list using .tolist()):\")\nprint(column_names_list)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load dataset\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\n\n# Drop rows with missing values in selected columns\npenguins = penguins[['species', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()\n\n# Set style\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# 1. Scatter Plot: Bill Length vs Bill Depth\nplt.figure(figsize=(8, 6))\nfor species in penguins['species'].unique():\n    subset = penguins[penguins['species'] == species]\n    plt.scatter(subset['bill_length_mm'], subset['bill_depth_mm'], label=species, alpha=0.7)\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Bill Depth (mm)\")\nplt.title(\"Bill Length vs Bill Depth by Species\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 2. Box Plot: Flipper Length by Species\nplt.figure(figsize=(8, 6))\npenguins.boxplot(column='flipper_length_mm', by='species')\nplt.title(\"Flipper Length by Species\")\nplt.suptitle('')\nplt.xlabel(\"Species\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.tight_layout()\nplt.show()\n\n# 3. Scatter Plot: Flipper Length vs Body Mass\nplt.figure(figsize=(8, 6))\nfor species in penguins['species'].unique():\n    subset = penguins[penguins['species'] == species]\n    plt.scatter(subset['flipper_length_mm'], subset['body_mass_g'], label=species, alpha=0.7)\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.title(\"Flipper Length vs Body Mass by Species\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport os\n\n# --- Detect if in Jupyter Notebook ---\ntry:\n    shell = get_ipython().__class__.__name__\n    if \"ZMQInteractiveShell\" in shell:\n        pio.renderers.default = 'notebook'  # Jupyter Notebook\n    else:\n        pio.renderers.default = 'iframe'    # Other notebook-like interface\nexcept NameError:\n    # Not in Jupyter (likely script or terminal)\n    try:\n        pio.renderers.default = 'browser'\n    except:\n        pio.renderers.default = 'svg'\n\n# --- Load dataset ---\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\n\n# --- Clean data ---\npenguins = penguins[['species', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()\n\n# --- Plot 1: Bill Length vs Bill Depth ---\nfig1 = px.scatter(\n    penguins,\n    x='bill_length_mm',\n    y='bill_depth_mm',\n    color='species',\n    title='Bill Length vs Bill Depth by Species',\n    labels={'bill_length_mm': 'Bill Length (mm)', 'bill_depth_mm': 'Bill Depth (mm)'}\n)\n\n# --- Plot 2: Flipper Length by Species (Box Plot) ---\nfig2 = px.box(\n    penguins,\n    x='species',\n    y='flipper_length_mm',\n    color='species',\n    title='Flipper Length by Species',\n    labels={'flipper_length_mm': 'Flipper Length (mm)'}\n)\n\n# --- Plot 3: Flipper Length vs Body Mass ---\nfig3 = px.scatter(\n    penguins,\n    x='flipper_length_mm',\n    y='body_mass_g',\n    color='species',\n    title='Flipper Length vs Body Mass by Species',\n    labels={'flipper_length_mm': 'Flipper Length (mm)', 'body_mass_g': 'Body Mass (g)'}\n)\n\n# --- Show or Save Plots Based on Environment ---\ntry:\n    # Try to show plots interactively\n    fig1.show()\n    fig2.show()\n    fig3.show()\nexcept:\n    # If that fails (e.g., headless), save as HTML\n    print(\"Interactive display failed. Saving plots as HTML...\")\n    fig1.write_html(\"fig1_bill_vs_depth.html\")\n    fig2.write_html(\"fig2_flipper_box.html\")\n    fig3.write_html(\"fig3_flipper_vs_mass.html\")\n    print(\"Plots saved in current directory.\")\n\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom IPython.display import HTML, display\n\n# Load and clean data\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\npenguins = penguins[['species', 'bill_length_mm', 'bill_depth_mm']].dropna()\n\n# Create plot\nfig = px.scatter(\n    penguins,\n    x='bill_length_mm',\n    y='bill_depth_mm',\n    color='species',\n    title='Bill Length vs Bill Depth by Species'\n)\n\n# Convert Plotly figure to HTML string\nhtml_plot = pio.to_html(fig, full_html=False, include_plotlyjs='cdn')\n\n# Display using IPython\ndisplay(HTML(html_plot))\n\n\nfig.write_html(\"my_plot.html\")\n#<iframe src=\"my_plot.html\" width=\"100%\" height=\"500px\"></iframe>\n\n# 📌 Step 1: Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# 📌 Step 2: Load Palmer Penguins Dataset\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\n\n# Drop rows with missing values\npenguins.dropna(inplace=True)\n\n# Display first few rows\npenguins.head()\n\n\n# 📌 Step 3: Exploratory Data Analysis (Optional but helpful)\nsns.pairplot(penguins, hue=\"species\")\nplt.suptitle(\"Penguin Feature Distributions by Species\", y=1.02)\nplt.show()\n\n\n# 📌 Step 4: Preprocessing\n# Encode categorical features: 'island', 'sex', and target 'species'\n\nle_species = LabelEncoder()\npenguins['species_label'] = le_species.fit_transform(penguins['species'])\n\nle_island = LabelEncoder()\npenguins['island_label'] = le_island.fit_transform(penguins['island'])\n\nle_sex = LabelEncoder()\npenguins['sex_label'] = le_sex.fit_transform(penguins['sex'])\n\n# Define features and target\nfeatures = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island_label', 'sex_label']\ntarget = 'species_label'\n\n\n# 📌 Step 5: Train-Test Split\nX = penguins[features]\ny = penguins[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# 📌 Step 6: Train Random Forest Classifier\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n\n# 📌 Step 7: Evaluate Model\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=le_species.classes_))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=le_species.classes_, yticklabels=le_species.classes_)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\n# 📌 Step 8: Feature Importance\nimportances = model.feature_importances_\nfeature_names = X.columns\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x=importances, y=feature_names)\nplt.title(\"Feature Importances in Penguin Classification\")\nplt.show()\n","type":"content","url":"/notebooks/ftuluri01","position":1},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡"},"type":"lvl1","url":"/notebooks/overview","position":0},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡"},"content":"Your Interactive Journey from Data Novice to AI Integration Expert\n\n🎯 Your Mission\n\nWelcome to your AI integration journey! Using the adorable Palmer Penguins dataset, you’ll master the complete data science workflow. By the end of this cookbook, you’ll have the skills and confidence to integrate AI into your own projects and workflows. Ready to become an AI Integration Expert?","type":"content","url":"/notebooks/overview","position":1},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl3":"📊 Progress Tracker"},"type":"lvl3","url":"/notebooks/overview#id-progress-tracker","position":2},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl3":"📊 Progress Tracker"},"content":"Progress \n\n0/4 Complete\n\n\n\n🎯 Current Status: Getting Started","type":"content","url":"/notebooks/overview#id-progress-tracker","position":3},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl3":"🗺️ Learning Path"},"type":"lvl3","url":"/notebooks/overview#id-learning-path","position":4},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl3":"🗺️ Learning Path"},"content":"","type":"content","url":"/notebooks/overview#id-learning-path","position":5},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl4":"🎯 Section 1: Project Overview","lvl3":"🗺️ Learning Path"},"type":"lvl4","url":"/notebooks/overview#id-section-1-project-overview","position":6},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl4":"🎯 Section 1: Project Overview","lvl3":"🗺️ Learning Path"},"content":"Understanding the Palmer Penguins Challenge\n\nWhat You’ll Accomplish:\n\nMeet our penguin friends (Adelie, Chinstrap, Gentoo)\n\nUnderstand the business problem: species classification\n\nReview the complete data science workflow\n\nSet your learning objectives\n\nReward: ⭐ Data Science Detective BadgeTime: ⏱️ 15 minutes\n\n[ ] Complete\n\n📖 Section Details\n\nLearn about our dataset, goals, and the AI workflow we’ll master together. This foundational section sets the stage for your entire learning journey.\n\nKey Concepts:\n\nData Science Methodology\n\nProblem Definition\n\nSuccess Metrics","type":"content","url":"/notebooks/overview#id-section-1-project-overview","position":7},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl4":"🔍 Section 2: Data Acquisition & Cleaning","lvl3":"🗺️ Learning Path"},"type":"lvl4","url":"/notebooks/overview#id-section-2-data-acquisition-cleaning","position":8},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl4":"🔍 Section 2: Data Acquisition & Cleaning","lvl3":"🗺️ Learning Path"},"content":"Getting Your Hands Dirty with Real Data\n\nWhat You’ll Accomplish:\n\nInstall and import the palmerpenguins package\n\nLoad the dataset and explore its structure\n\nIdentify and handle missing values\n\nPerform basic data quality checks\n\nCreate a clean dataset ready for analysis\n\nReward: ⭐ Data Cleaning Specialist CertificateTime: ⏱️ 45 minutes\n\n[ ] Complete\n\n🔧 Section Details\n\nMaster the art of loading, exploring, and cleaning messy real-world data. You’ll learn essential data preprocessing skills.\n\nKey Skills:\n\nData Loading Techniques\n\nMissing Value Handling\n\nData Quality Assessment","type":"content","url":"/notebooks/overview#id-section-2-data-acquisition-cleaning","position":9},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl4":"📊 Section 3: Data Visualization & Exploration","lvl3":"🗺️ Learning Path"},"type":"lvl4","url":"/notebooks/overview#id-section-3-data-visualization-exploration","position":10},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl4":"📊 Section 3: Data Visualization & Exploration","lvl3":"🗺️ Learning Path"},"content":"Making Data Tell Its Story\n\nWhat You’ll Accomplish:\n\nCreate species distribution plots\n\nBuild correlation heatmaps\n\nDesign interactive scatter plots\n\nCraft publication-ready visualizations\n\nDiscover surprising patterns in penguin behavior\n\nReward: ⭐ Visualization Virtuoso MedalTime: ⏱️ 60 minutes\n\n[ ] Complete\n\n📈 Section Details\n\nCreate compelling visualizations that reveal hidden patterns and insights. Transform raw data into meaningful stories.\n\nVisualization Types:\n\nStatistical Distributions\n\nCorrelation Analysis\n\nInteractive Plots","type":"content","url":"/notebooks/overview#id-section-3-data-visualization-exploration","position":11},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl4":"🧠 Section 4: AI Model Building & Prediction","lvl3":"🗺️ Learning Path"},"type":"lvl4","url":"/notebooks/overview#id-section-4-ai-model-building-prediction","position":12},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl4":"🧠 Section 4: AI Model Building & Prediction","lvl3":"🗺️ Learning Path"},"content":"Building Your First AI Classification System\n\nWhat You’ll Accomplish:\n\nSplit data into training and testing sets\n\nTrain your first decision tree classifier\n\nCompare multiple AI algorithms\n\nOptimize model performance\n\nDeploy your AI system for real predictions\n\nReward: ⭐ AI Integration Master CertificationTime: ⏱️ 90 minutes\n\n[ ] Complete\n\n🤖 Section Details\n\nTrain machine learning models to automatically classify penguin species. Build your first complete AI system from scratch.\n\nML Techniques:\n\nTrain-Test Splitting\n\nAlgorithm Comparison\n\nPerformance Optimization\n\n","type":"content","url":"/notebooks/overview#id-section-4-ai-model-building-prediction","position":13},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl2":"🎉 Congratulations! 🏆"},"type":"lvl2","url":"/notebooks/overview#id-congratulations","position":14},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl2":"🎉 Congratulations! 🏆"},"content":"You are now AI Integration Certified!\n\nYou’ve mastered the complete data science workflow and are ready to integrate AI into your own projects. Share your achievement and help others on their AI journey!","type":"content","url":"/notebooks/overview#id-congratulations","position":15},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl3":"🚀 Ready to Begin?","lvl2":"🎉 Congratulations! 🏆"},"type":"lvl3","url":"/notebooks/overview#id-ready-to-begin","position":16},{"hierarchy":{"lvl1":"⚡ AI Integration Cookbook ⚡","lvl3":"🚀 Ready to Begin?","lvl2":"🎉 Congratulations! 🏆"},"content":"📚 Prerequisites\n\nBasic Python knowledge (helpful but not required)\n\nJupyter Notebook environment\n\nCuriosity and willingness to learn!\n\n🎁 What You’ll Get\n\nHands-on experience with real data\n\nReusable code templates\n\nConfidence to tackle your own AI projects\n```{button-link} #\n:color: primary\n:expand:\n:click-parent:\n\n🐧 Start Your Penguin Adventure!\n\n💡 Pro Tip\n\nClick the checkboxes as you complete each section to track your progress and unlock achievement badges!","type":"content","url":"/notebooks/overview#id-ready-to-begin","position":17}]}