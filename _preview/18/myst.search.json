{"version":"1","records":[{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nSee the \n\nCookbook Contributor‚Äôs Guide for step-by-step instructions on how to create your new Cookbook and get it hosted on the \n\nPythia Cookbook Gallery!\n\nThis Project Pythia Cookbook covers ... (replace ... with the main subject of your cookbook ... e.g., working with radar data in Python)","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"Enhancing Algorithmic Literacy Among Vulnerable Populations through Geoscience Applications"},"type":"lvl2","url":"/#enhancing-algorithmic-literacy-among-vulnerable-populations-through-geoscience-applications","position":2},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"Enhancing Algorithmic Literacy Among Vulnerable Populations through Geoscience Applications"},"content":"Imagine a community-based geoscience project where local learners explore biodiversity through a dataset of penguin species.\n\nIn the geoscience community, algorithm literacy is more than technical fluency‚Äîit‚Äôs a pathway to justice. Vulnerable populations often bear the brunt of environmental decisions shaped by opaque data systems. To shift this paradigm, we must equip these communities not only with knowledge, but with agency.\n\nEnter the AI teammate:  not as a distant tool, but as a responsive partner! Through explicit AI prompts, learners can engage in targeted, conversational learning that breaks down complex algorithmic concepts into accessible, actionable insights. These prompts serve as microburst upskilling lessons--short, focused interactions where AI explains cocepts like correlations, classification, and feature importance as plain language.\n\nVulnerable populations, often excluded from technical discourse, now gain the tools to explore, question, and interpret data that mirrors real-world ecological systems - tailored to real-world geoscience challenges.\n\nBy embedding microburst upskilling into daily workflows and community initiatives, we create rapid, scalable opportunities for learning. These short, focused interactions with AI empower users to ask better questions, challenge assumptions, and co-create solutions. The result? A growing network of algorithm-literate individuals who can actively participate in‚Äîand shape‚Äîtheir own liberation.\n\nLet us reimagine algorithm literacy not as a distant goal, but as a daily practice. With AI as a teammate and microburst learning as our method, we can build a geoscience future that is inclusive, transparent, and driven by the voices that matter most.","type":"content","url":"/#enhancing-algorithmic-literacy-among-vulnerable-populations-through-geoscience-applications","position":3},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"Authors"},"content":"Lead Author, \n\nFirst Author, \n\nSecond Author, \n\nThird Author","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"Structure"},"content":"(State one or more sections that will comprise the notebook. E.g., This cookbook is broken up into two main sections - ‚ÄúFoundations‚Äù and ‚ÄúExample Workflows.‚Äù Then, describe each section below.)","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Section 1 ( Replace with the title of this section, e.g. ‚ÄúFoundations‚Äù )","lvl2":"Structure"},"type":"lvl3","url":"/#section-1-replace-with-the-title-of-this-section-e-g-foundations","position":10},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Section 1 ( Replace with the title of this section, e.g. ‚ÄúFoundations‚Äù )","lvl2":"Structure"},"content":"(Add content for this section, e.g., \"The foundational content includes ... \")","type":"content","url":"/#section-1-replace-with-the-title-of-this-section-e-g-foundations","position":11},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Section 2 ( Replace with the title of this section, e.g. ‚ÄúExample workflows‚Äù )","lvl2":"Structure"},"type":"lvl3","url":"/#section-2-replace-with-the-title-of-this-section-e-g-example-workflows","position":12},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Section 2 ( Replace with the title of this section, e.g. ‚ÄúExample workflows‚Äù )","lvl2":"Structure"},"content":"(Add content for this section, e.g., \"Example workflows include ... \")","type":"content","url":"/#section-2-replace-with-the-title-of-this-section-e-g-example-workflows","position":13},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":14},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":15},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":16},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n‚Äúlaunch Binder‚Äù. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you‚Äôll be able to execute\nand even change the example programs. You‚Äôll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":17},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace ‚Äúcookbook-example‚Äù with the title of your cookbooks)\n\nClone the https://github.com/ProjectPythia/cookbook-example repository: git clone https://github.com/ProjectPythia/cookbook-example.git\n\nMove into the cookbook-example directorycd cookbook-example\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cookbook-example\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Conclusion","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#conclusion","position":20},{"hierarchy":{"lvl1":"A Simplified Data Analysis Using an AI Teammate Exploration Cookbook","lvl3":"Conclusion","lvl2":"Running the Notebooks"},"content":"This project demonstrates that data science is an accessible and impactful discipline that can be applied across a wide range of fields, including scientific research, industrial operations, government, private industry, and manufacturing/production. By emphasizing practical experience, task-specific prompts, and reproducible workflows, we highlight the importance of learning by doing‚Äîallowing participants to track progress through cumulative task-specific badges that culminate in context-based credentials.\n\nA key takeaway from our approach is that no prior coding experience is required to engage meaningfully with data science. Through community development and upskilling, individuals can bridge the gap between technical barriers and real-world applications, empowering broader participation in the data-driven economy.\n\nUltimately, this project underscores a democratized vision of data science‚Äîwhere anyone can learn, practice, and contribute‚Äîby fostering inclusive learning environments, sharing reproducible tools, and promoting AI-integrated pathways such as the AI integration specialist badge. We hope this work serves as a foundation for scalable, interdisciplinary engagement and sustainable growth in digital literacy and data fluency.","type":"content","url":"/#conclusion","position":21},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/notebooks/ftuluri01","position":0},{"hierarchy":{"lvl1":""},"content":"import pandas as pd\n\n# GitHub URL for the CSV file (raw format)\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n\n# Load the dataset into a pandas DataFrame\npenguins = pd.read_csv(url)\n\n# Display the variable (column) names\nprint(\"Variable (column) names in the Palmer Penguins dataset:\")\nprint(penguins.columns.tolist())\n\npip install plotly\n\nimport plotly\n\nimport pandas as pd\n\n# GitHub URL for the CSV file (raw format)\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n\n# Load the dataset into a pandas DataFrame\npenguins = pd.read_csv(url)\n\n# Display the variable (column) names\nprint(\"Variable (column) names in the Palmer Penguins dataset:\")\nprint(penguins.columns.tolist())\n\n\nprint(penguins.head(10))\n\ncolumn_names_list = penguins.columns.tolist()\nprint(\"Column names (as a list using .tolist()):\")\nprint(column_names_list)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load dataset\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\n\n# Drop rows with missing values in selected columns\npenguins = penguins[['species', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()\n\n# Set style\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# 1. Scatter Plot: Bill Length vs Bill Depth\nplt.figure(figsize=(8, 6))\nfor species in penguins['species'].unique():\n    subset = penguins[penguins['species'] == species]\n    plt.scatter(subset['bill_length_mm'], subset['bill_depth_mm'], label=species, alpha=0.7)\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Bill Depth (mm)\")\nplt.title(\"Bill Length vs Bill Depth by Species\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 2. Box Plot: Flipper Length by Species\nplt.figure(figsize=(8, 6))\npenguins.boxplot(column='flipper_length_mm', by='species')\nplt.title(\"Flipper Length by Species\")\nplt.suptitle('')\nplt.xlabel(\"Species\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.tight_layout()\nplt.show()\n\n# 3. Scatter Plot: Flipper Length vs Body Mass\nplt.figure(figsize=(8, 6))\nfor species in penguins['species'].unique():\n    subset = penguins[penguins['species'] == species]\n    plt.scatter(subset['flipper_length_mm'], subset['body_mass_g'], label=species, alpha=0.7)\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.title(\"Flipper Length vs Body Mass by Species\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport os\n\n# --- Detect if in Jupyter Notebook ---\ntry:\n    shell = get_ipython().__class__.__name__\n    if \"ZMQInteractiveShell\" in shell:\n        pio.renderers.default = 'notebook'  # Jupyter Notebook\n    else:\n        pio.renderers.default = 'iframe'    # Other notebook-like interface\nexcept NameError:\n    # Not in Jupyter (likely script or terminal)\n    try:\n        pio.renderers.default = 'browser'\n    except:\n        pio.renderers.default = 'svg'\n\n# --- Load dataset ---\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\n\n# --- Clean data ---\npenguins = penguins[['species', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()\n\n# --- Plot 1: Bill Length vs Bill Depth ---\nfig1 = px.scatter(\n    penguins,\n    x='bill_length_mm',\n    y='bill_depth_mm',\n    color='species',\n    title='Bill Length vs Bill Depth by Species',\n    labels={'bill_length_mm': 'Bill Length (mm)', 'bill_depth_mm': 'Bill Depth (mm)'}\n)\n\n# --- Plot 2: Flipper Length by Species (Box Plot) ---\nfig2 = px.box(\n    penguins,\n    x='species',\n    y='flipper_length_mm',\n    color='species',\n    title='Flipper Length by Species',\n    labels={'flipper_length_mm': 'Flipper Length (mm)'}\n)\n\n# --- Plot 3: Flipper Length vs Body Mass ---\nfig3 = px.scatter(\n    penguins,\n    x='flipper_length_mm',\n    y='body_mass_g',\n    color='species',\n    title='Flipper Length vs Body Mass by Species',\n    labels={'flipper_length_mm': 'Flipper Length (mm)', 'body_mass_g': 'Body Mass (g)'}\n)\n\n# --- Show or Save Plots Based on Environment ---\ntry:\n    # Try to show plots interactively\n    fig1.show()\n    fig2.show()\n    fig3.show()\nexcept:\n    # If that fails (e.g., headless), save as HTML\n    print(\"Interactive display failed. Saving plots as HTML...\")\n    fig1.write_html(\"fig1_bill_vs_depth.html\")\n    fig2.write_html(\"fig2_flipper_box.html\")\n    fig3.write_html(\"fig3_flipper_vs_mass.html\")\n    print(\"Plots saved in current directory.\")\n\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom IPython.display import HTML, display\n\n# Load and clean data\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\npenguins = penguins[['species', 'bill_length_mm', 'bill_depth_mm']].dropna()\n\n# Create plot\nfig = px.scatter(\n    penguins,\n    x='bill_length_mm',\n    y='bill_depth_mm',\n    color='species',\n    title='Bill Length vs Bill Depth by Species'\n)\n\n# Convert Plotly figure to HTML string\nhtml_plot = pio.to_html(fig, full_html=False, include_plotlyjs='cdn')\n\n# Display using IPython\ndisplay(HTML(html_plot))\n\n\nfig.write_html(\"my_plot.html\")\n#<iframe src=\"my_plot.html\" width=\"100%\" height=\"500px\"></iframe>\n\n# üìå Step 1: Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# üìå Step 2: Load Palmer Penguins Dataset\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\n\n# Drop rows with missing values\npenguins.dropna(inplace=True)\n\n# Display first few rows\npenguins.head()\n\n\n# üìå Step 3: Exploratory Data Analysis (Optional but helpful)\nsns.pairplot(penguins, hue=\"species\")\nplt.suptitle(\"Penguin Feature Distributions by Species\", y=1.02)\nplt.show()\n\n\n# üìå Step 4: Preprocessing\n# Encode categorical features: 'island', 'sex', and target 'species'\n\nle_species = LabelEncoder()\npenguins['species_label'] = le_species.fit_transform(penguins['species'])\n\nle_island = LabelEncoder()\npenguins['island_label'] = le_island.fit_transform(penguins['island'])\n\nle_sex = LabelEncoder()\npenguins['sex_label'] = le_sex.fit_transform(penguins['sex'])\n\n# Define features and target\nfeatures = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island_label', 'sex_label']\ntarget = 'species_label'\n\n\n# üìå Step 5: Train-Test Split\nX = penguins[features]\ny = penguins[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# üìå Step 6: Train Random Forest Classifier\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n\n# üìå Step 7: Evaluate Model\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=le_species.classes_))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=le_species.classes_, yticklabels=le_species.classes_)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\n# üìå Step 8: Feature Importance\nimportances = model.feature_importances_\nfeature_names = X.columns\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x=importances, y=feature_names)\nplt.title(\"Feature Importances in Penguin Classification\")\nplt.show()\n","type":"content","url":"/notebooks/ftuluri01","position":1},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°"},"type":"lvl1","url":"/notebooks/overview","position":0},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°"},"content":"Your Interactive Journey from Data Novice to AI Integration Expert\n\nüéØ Your Mission\n\nWelcome to your AI integration journey! Using the adorable Palmer Penguins dataset, you‚Äôll master the complete data science workflow. By the end of this cookbook, you‚Äôll have the skills and confidence to integrate AI into your own projects and workflows. Ready to become an AI Integration Expert?","type":"content","url":"/notebooks/overview","position":1},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl3":"üìä Progress Tracker"},"type":"lvl3","url":"/notebooks/overview#id-progress-tracker","position":2},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl3":"üìä Progress Tracker"},"content":"Progress \n\n0/4 Complete\n\n\n\nüéØ Current Status: Getting Started","type":"content","url":"/notebooks/overview#id-progress-tracker","position":3},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl3":"üó∫Ô∏è Learning Path"},"type":"lvl3","url":"/notebooks/overview#id-learning-path","position":4},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl3":"üó∫Ô∏è Learning Path"},"content":"","type":"content","url":"/notebooks/overview#id-learning-path","position":5},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl4":"üéØ Section 1: Project Overview","lvl3":"üó∫Ô∏è Learning Path"},"type":"lvl4","url":"/notebooks/overview#id-section-1-project-overview","position":6},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl4":"üéØ Section 1: Project Overview","lvl3":"üó∫Ô∏è Learning Path"},"content":"Understanding the Palmer Penguins Challenge\n\nWhat You‚Äôll Accomplish:\n\nMeet our penguin friends (Adelie, Chinstrap, Gentoo)\n\nUnderstand the business problem: species classification\n\nReview the complete data science workflow\n\nSet your learning objectives\n\nReward: ‚≠ê Data Science Detective BadgeTime: ‚è±Ô∏è 15 minutes\n\n[ ] Complete\n\nüìñ Section Details\n\nLearn about our dataset, goals, and the AI workflow we‚Äôll master together. This foundational section sets the stage for your entire learning journey.\n\nKey Concepts:\n\nData Science Methodology\n\nProblem Definition\n\nSuccess Metrics","type":"content","url":"/notebooks/overview#id-section-1-project-overview","position":7},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl4":"üîç Section 2: Data Acquisition & Cleaning","lvl3":"üó∫Ô∏è Learning Path"},"type":"lvl4","url":"/notebooks/overview#id-section-2-data-acquisition-cleaning","position":8},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl4":"üîç Section 2: Data Acquisition & Cleaning","lvl3":"üó∫Ô∏è Learning Path"},"content":"Getting Your Hands Dirty with Real Data\n\nWhat You‚Äôll Accomplish:\n\nInstall and import the palmerpenguins package\n\nLoad the dataset and explore its structure\n\nIdentify and handle missing values\n\nPerform basic data quality checks\n\nCreate a clean dataset ready for analysis\n\nReward: ‚≠ê Data Cleaning Specialist CertificateTime: ‚è±Ô∏è 45 minutes\n\n[ ] Complete\n\nüîß Section Details\n\nMaster the art of loading, exploring, and cleaning messy real-world data. You‚Äôll learn essential data preprocessing skills.\n\nKey Skills:\n\nData Loading Techniques\n\nMissing Value Handling\n\nData Quality Assessment","type":"content","url":"/notebooks/overview#id-section-2-data-acquisition-cleaning","position":9},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl4":"üìä Section 3: Data Visualization & Exploration","lvl3":"üó∫Ô∏è Learning Path"},"type":"lvl4","url":"/notebooks/overview#id-section-3-data-visualization-exploration","position":10},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl4":"üìä Section 3: Data Visualization & Exploration","lvl3":"üó∫Ô∏è Learning Path"},"content":"Making Data Tell Its Story\n\nWhat You‚Äôll Accomplish:\n\nCreate species distribution plots\n\nBuild correlation heatmaps\n\nDesign interactive scatter plots\n\nCraft publication-ready visualizations\n\nDiscover surprising patterns in penguin behavior\n\nReward: ‚≠ê Visualization Virtuoso MedalTime: ‚è±Ô∏è 60 minutes\n\n[ ] Complete\n\nüìà Section Details\n\nCreate compelling visualizations that reveal hidden patterns and insights. Transform raw data into meaningful stories.\n\nVisualization Types:\n\nStatistical Distributions\n\nCorrelation Analysis\n\nInteractive Plots","type":"content","url":"/notebooks/overview#id-section-3-data-visualization-exploration","position":11},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl4":"üß† Section 4: AI Model Building & Prediction","lvl3":"üó∫Ô∏è Learning Path"},"type":"lvl4","url":"/notebooks/overview#id-section-4-ai-model-building-prediction","position":12},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl4":"üß† Section 4: AI Model Building & Prediction","lvl3":"üó∫Ô∏è Learning Path"},"content":"Building Your First AI Classification System\n\nWhat You‚Äôll Accomplish:\n\nSplit data into training and testing sets\n\nTrain your first decision tree classifier\n\nCompare multiple AI algorithms\n\nOptimize model performance\n\nDeploy your AI system for real predictions\n\nReward: ‚≠ê AI Integration Master CertificationTime: ‚è±Ô∏è 90 minutes\n\n[ ] Complete\n\nü§ñ Section Details\n\nTrain machine learning models to automatically classify penguin species. Build your first complete AI system from scratch.\n\nML Techniques:\n\nTrain-Test Splitting\n\nAlgorithm Comparison\n\nPerformance Optimization\n\n","type":"content","url":"/notebooks/overview#id-section-4-ai-model-building-prediction","position":13},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl2":"üéâ Congratulations! üèÜ"},"type":"lvl2","url":"/notebooks/overview#id-congratulations","position":14},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl2":"üéâ Congratulations! üèÜ"},"content":"You are now AI Integration Certified!\n\nYou‚Äôve mastered the complete data science workflow and are ready to integrate AI into your own projects. Share your achievement and help others on their AI journey!","type":"content","url":"/notebooks/overview#id-congratulations","position":15},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl3":"üöÄ Ready to Begin?","lvl2":"üéâ Congratulations! üèÜ"},"type":"lvl3","url":"/notebooks/overview#id-ready-to-begin","position":16},{"hierarchy":{"lvl1":"‚ö° AI Integration Cookbook ‚ö°","lvl3":"üöÄ Ready to Begin?","lvl2":"üéâ Congratulations! üèÜ"},"content":"üìö Prerequisites\n\nBasic Python knowledge (helpful but not required)\n\nJupyter Notebook environment\n\nCuriosity and willingness to learn!\n\nüéÅ What You‚Äôll Get\n\nHands-on experience with real data\n\nReusable code templates\n\nConfidence to tackle your own AI projects\n```{button-link} #\n:color: primary\n:expand:\n:click-parent:\n\nüêß Start Your Penguin Adventure!\n\nüí° Pro Tip\n\nClick the checkboxes as you complete each section to track your progress and unlock achievement badges!","type":"content","url":"/notebooks/overview#id-ready-to-begin","position":17},{"hierarchy":{"lvl1":"üêß Palmer Penguins: A Data Science Recipe"},"type":"lvl1","url":"/notebooks/palmer-penguins-recipe","position":0},{"hierarchy":{"lvl1":"üêß Palmer Penguins: A Data Science Recipe"},"content":"This notebook demonstrates how to explore and analyze the Palmer Penguins dataset using Pandas and Matplotlib. You‚Äôll learn how to clean data, visualize relationships, and prepare for modeling.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nurl = \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\npenguins = pd.read_csv(url)\npenguins.head()\n\n","type":"content","url":"/notebooks/palmer-penguins-recipe","position":1},{"hierarchy":{"lvl1":"üêß Palmer Penguins: A Data Science Recipe","lvl2":"üßπ Step 1: Data Cleaning"},"type":"lvl2","url":"/notebooks/palmer-penguins-recipe#id-step-1-data-cleaning","position":2},{"hierarchy":{"lvl1":"üêß Palmer Penguins: A Data Science Recipe","lvl2":"üßπ Step 1: Data Cleaning"},"content":"We‚Äôll check for missing values and clean the dataset by removing or filling them.\n\npenguins.info()\npenguins.isnull().sum()\n\n# Drop rows with missing values\npenguins_clean = penguins.dropna()\npenguins_clean.shape\n\n","type":"content","url":"/notebooks/palmer-penguins-recipe#id-step-1-data-cleaning","position":3},{"hierarchy":{"lvl1":"üêß Palmer Penguins: A Data Science Recipe","lvl2":"üìä Step 2: Visualizations with Matplotlib"},"type":"lvl2","url":"/notebooks/palmer-penguins-recipe#id-step-2-visualizations-with-matplotlib","position":4},{"hierarchy":{"lvl1":"üêß Palmer Penguins: A Data Science Recipe","lvl2":"üìä Step 2: Visualizations with Matplotlib"},"content":"Let‚Äôs visualize the distribution of penguin species and relationships between features.\n\n# Bar chart of species count\npenguins_clean['species'].value_counts().plot(kind='bar', title='Species Count')\nplt.xlabel('Species')\nplt.ylabel('Count')\nplt.show()\n\n# Scatter plot: Bill Length vs Bill Depth\nfor species in penguins_clean['species'].unique():\n    subset = penguins_clean[penguins_clean['species'] == species]\n    plt.scatter(subset['bill_length_mm'], subset['bill_depth_mm'], label=species)\n\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Bill Length vs Bill Depth by Species')\nplt.legend()\nplt.show()","type":"content","url":"/notebooks/palmer-penguins-recipe#id-step-2-visualizations-with-matplotlib","position":5}]}